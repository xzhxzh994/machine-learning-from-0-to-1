{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a8d335",
   "metadata": {},
   "source": [
    "# 线性回归：\n",
    "$$ Y = X  \\cdot W + b$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa6ae8",
   "metadata": {},
   "source": [
    "## 在实现的时候，我们需要加入误差  \n",
    "即:\n",
    "$$ Y = X  \\cdot W + b + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cfc68",
   "metadata": {},
   "source": [
    "对于一个给定的训练集:\n",
    "$$[X,Y]$$\n",
    "其中X和Y一一对应  \n",
    "我们需要做的是以下几步:  \n",
    "一.预处理数据  \n",
    "包括:清理空值、标签对应等等  \n",
    "最重要的: 定义 **batch_size** ,按照 **batch_size** 进行随机取样  \n",
    "这样我们就得到了若干X-Y样本\n",
    "$$[X_1,Y_1][X_2,Y_2]\\dots[X_i,Y_i] \\quad where\\;i = 1,2,\\dots n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b869084",
   "metadata": {},
   "source": [
    "之后我们根据 原始回归函数的输出得到 $\\hat{Y}$  \n",
    "  \n",
    "# Loss  \n",
    "对每个batch来说 均方误:  \n",
    "$$ loss\\quad=\\; \\sum_{i=1}^{i=n} \\hat{y_i} - y_i \\quad where \\; \\hat{y_i},y_i\\quad in \\quad \\hat{Y},Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b194e8",
   "metadata": {},
   "source": [
    "# SGD  \n",
    "优化的本质:调整公式中的参数，让下一次训练产生的**损失更低**\n",
    "方法:对损失求导，得到梯度grad，让参数-lr*梯度，得到新的参数值,以w为例:\n",
    "$$w^n = w^n-1 - lr \\cdot grad $$\n",
    "\n",
    "考虑到我们的grad是一整个batch积累的总的值，我们要对他求均值，所以是：\n",
    "$$w^n = w^n-1 - lr \\cdot \\bar{grad} $$\n",
    "其中 $$ \\bar{grad} = \\frac{\\sum_{i = 1}^{i=batch\\_size} grad_i}{batch\\_size}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96b40e",
   "metadata": {},
   "source": [
    "# epoch  \n",
    "为了不断优化，我们应该重复以上操作 若干个epoch  \n",
    "```pseudocode\n",
    "for each_epoch\n",
    "    for each_batch\n",
    "        do cur_y = func(X)\n",
    "        loss = Loss_comput(cur_y,y)\n",
    "        w,b = SGD(lr,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f680c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "#实战\n",
    "#我们希望得到一个 y = Wx + b + sigma\n",
    "def create_data(w,b,num_examples):\n",
    "    #w的size是对应了特征维度的偏差\n",
    "    #b也是\n",
    "    #num_examples是样本数量\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))#先生成X\n",
    "    y = torch.matmul(X,w)+b#根据x生成y//这里用的是matmul，所以X*w变成了一列，但是是一维的，我们希望它是二维的\n",
    "    y += torch.normal(0,0.01,y.shape)#在加上偏差\n",
    "    return X,y.reshape((-1,1))\n",
    "    #y.reshape((-1, 1)) 确保标签 y 是列向量（形状为 (样本数, 1)），方便后续与模型输出（通常也是列向量）计算损失。\n",
    "    #而 -1 是一个特殊值，它的含义是：“根据另一个维度的大小和总元素数，自动计算当前维度的大小”。\n",
    "\n",
    "#初始化真实的数据\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = create_data(true_w,true_b,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82f266a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机初始化一个b和w\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据分批处理函数\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)  # 获取总样本数（如1000个样本）\n",
    "    indices = list(range(num_examples))  # 生成样本索引列表（如[0,1,2,...,999]）\n",
    "    # 随机打乱索引：让样本顺序随机化，避免模型学习到数据顺序的规律\n",
    "    random.shuffle(indices)\n",
    "    # 按批次遍历所有样本：从0开始，每次跳batch_size步\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 取当前批次的索引：从i到i+batch_size（最后一批可能不足batch_size，用min处理）\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)]\n",
    "        )\n",
    "        # 用生成器（yield）返回当前批次的特征和标签\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0632f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义函数\n",
    "def linreg(X,w,b):\n",
    "    return torch.matmul(X,w) + b\n",
    "#定义损失\n",
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
    "#定义sgd\n",
    "def sgd(params,lr,batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    \"\"\"所谓随机指的是每次更新的数据样本是随机选择的，而不是固定选择的\"\"\"\n",
    "    with torch.no_grad():#临时关闭张量的梯度计算功能\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb82297",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d474fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss 0.000050\n",
      "epoch 2,loss 0.000049\n",
      "epoch 3,loss 0.000049\n"
     ]
    }
   ],
   "source": [
    "#定义一些基本的\n",
    "lr = 0.3\n",
    "num_epoch = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "#已知总数据大小是1000，我们取每批10个\n",
    "batch_size = 10\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter(batch_size=batch_size,features = features,labels = labels):\n",
    "        l = loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_loss = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1},loss {float (train_loss.mean()):f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs149",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
