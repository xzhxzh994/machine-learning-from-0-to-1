{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a8d335",
   "metadata": {},
   "source": [
    "# 线性回归：\n",
    "$$ Y = X  \\cdot W + b$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa6ae8",
   "metadata": {},
   "source": [
    "## 在实现的时候，我们需要加入误差  \n",
    "即:\n",
    "$$ Y = X  \\cdot W + b + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cfc68",
   "metadata": {},
   "source": [
    "对于一个给定的训练集:\n",
    "$$[X,Y]$$\n",
    "其中X和Y一一对应  \n",
    "我们需要做的是以下几步:  \n",
    "一.预处理数据  \n",
    "包括:清理空值、标签对应等等  \n",
    "最重要的: 定义 **batch_size** ,按照 **batch_size** 进行随机取样  \n",
    "这样我们就得到了若干X-Y样本\n",
    "$$[X_1,Y_1][X_2,Y_2]\\dots[X_i,Y_i] \\quad where\\;i = 1,2,\\dots n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b869084",
   "metadata": {},
   "source": [
    "之后我们根据 原始回归函数的输出得到 $\\hat{Y}$  \n",
    "  \n",
    "# Loss  \n",
    "对每个batch来说 均方误:  \n",
    "$$ loss\\quad=\\; \\sum_{i=1}^{i=n} \\hat{y_i} - y_i \\quad where \\; \\hat{y_i},y_i\\quad in \\quad \\hat{Y},Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b194e8",
   "metadata": {},
   "source": [
    "# SGD  \n",
    "优化的本质:调整公式中的参数，让下一次训练产生的**损失更低**\n",
    "方法:对损失求导，得到梯度grad，让参数-lr*梯度，得到新的参数值,以w为例:\n",
    "$$w^n = w^n-1 - lr \\cdot grad $$\n",
    "\n",
    "考虑到我们的grad是一整个batch积累的总的值，我们要对他求均值，所以是：\n",
    "$$w^n = w^n-1 - lr \\cdot \\bar{grad} $$\n",
    "其中 $$ \\bar{grad} = \\frac{\\sum_{i = 1}^{i=batch\\_size} grad_i}{batch\\_size}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96b40e",
   "metadata": {},
   "source": [
    "# epoch  \n",
    "为了不断优化，我们应该重复以上操作 若干个epoch  \n",
    "```pseudocode\n",
    "for each_epoch\n",
    "    for each_batch\n",
    "        do cur_y = func(X)\n",
    "        loss = Loss_comput(cur_y,y)\n",
    "        w,b = SGD(lr,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f680c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "#实战\n",
    "#我们希望得到一个 y = Wx + b + sigma\n",
    "def create_data(w,b,num_examples):\n",
    "    #w的size是对应了特征维度的偏差\n",
    "    #b也是\n",
    "    #num_examples是样本数量\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))#先生成X\n",
    "    y = torch.matmul(X,w)+b#根据x生成y//这里用的是matmul，所以X*w变成了一列，但是是一维的，我们希望它是二维的\n",
    "    y += torch.normal(0,0.01,y.shape)#在加上偏差\n",
    "    return X,y.reshape((-1,1))\n",
    "    #y.reshape((-1, 1)) 确保标签 y 是列向量（形状为 (样本数, 1)），方便后续与模型输出（通常也是列向量）计算损失。\n",
    "    #而 -1 是一个特殊值，它的含义是：“根据另一个维度的大小和总元素数，自动计算当前维度的大小”。\n",
    "\n",
    "#初始化真实的数据\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = create_data(true_w,true_b,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82f266a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机初始化一个b和w\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据分批处理函数\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)  # 获取总样本数（如1000个样本）\n",
    "    indices = list(range(num_examples))  # 生成样本索引列表（如[0,1,2,...,999]）\n",
    "    # 随机打乱索引：让样本顺序随机化，避免模型学习到数据顺序的规律\n",
    "    random.shuffle(indices)\n",
    "    # 按批次遍历所有样本：从0开始，每次跳batch_size步\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 取当前批次的索引：从i到i+batch_size（最后一批可能不足batch_size，用min处理）\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)]\n",
    "        )\n",
    "        # 用生成器（yield）返回当前批次的特征和标签\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0632f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义函数\n",
    "def linreg(X,w,b):\n",
    "    return torch.matmul(X,w) + b\n",
    "#定义损失\n",
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
    "#定义sgd\n",
    "def sgd(params,lr,batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    \"\"\"所谓随机指的是每次更新的数据样本是随机选择的，而不是固定选择的\"\"\"\n",
    "    with torch.no_grad():#临时关闭张量的梯度计算功能\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb82297",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d474fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss 0.000050\n",
      "epoch 2,loss 0.000049\n",
      "epoch 3,loss 0.000049\n"
     ]
    }
   ],
   "source": [
    "#定义一些基本的\n",
    "lr = 0.3\n",
    "num_epoch = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "#已知总数据大小是1000，我们取每批10个\n",
    "batch_size = 10\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter(batch_size=batch_size,features = features,labels = labels):\n",
    "        l = loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_loss = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1},loss {float (train_loss.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98621e10",
   "metadata": {},
   "source": [
    "# pytorch有最简单的实现  \n",
    "线性回归:\n",
    "$$torch.nn.Linear(input_shape,output_shape)$$  \n",
    "损失函数:\n",
    "$$nn.MSELoss()$$\n",
    "SGD:\n",
    "$$torch.optim.SGD(net.parameters,lr)$$\n",
    "\n",
    "# 读取数据集\n",
    "我们可以调用框架中现有的API来读取数据。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5348cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    \"\"\"\n",
    "    TensorDataset 是 PyTorch 的一个数据集类，用于将多个张量（Tensor）打包成一个数据集。\n",
    "    这里的 *data_arrays 会解包输入的元组（比如 (features, labels) 会被拆成 features 和 labels 两个张量），\n",
    "    然后将它们按样本维度对齐（即第 i 个样本的特征和标签对应）。\n",
    "    \"\"\"\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "    \"\"\"\n",
    "    DataLoader 是 PyTorch 用于批量加载数据的迭代器，它会从 dataset 中按 batch_size 读取样本，\n",
    "    并且在 is_train=True 时（训练阶段）打乱数据顺序（shuffle=True），避免模型学习到数据的顺序规律。\n",
    "    \"\"\"\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "#之后我们可以使用next来获得data_iter中的每一批数据，或者使用in来获得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6cfa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4abe2de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们通过net[0]选择网络中的第一个图层\n",
    "#使用weight.data和bias.data方法访问参数\n",
    "#使用替换方法normal_和fill_来重写参数值\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057e5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51c2ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58f16a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000191\n",
      "epoch 2, loss 0.000098\n",
      "epoch 3, loss 0.000098\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()#注意要调用zero_grad来清空参数的权重\n",
    "        l.backward()\n",
    "        trainer.step()#进行更新\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs149",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
